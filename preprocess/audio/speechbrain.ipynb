{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "import speechbrain as sb\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "from IPython.display import Audio\n",
    "from scipy.io.wavfile import write as wavwrite\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import SepformerSeparation as separator\n",
    "import torchaudio\n",
    "from lared_dataset.constants import (processed_audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 8000\n",
    "valid_segments_ms = pickle.load(open('./valid_audio_segments.pkl', 'rb'))\n",
    "valid_segments_ms = [el[1] for el in valid_segments_ms]\n",
    "valid_segments_ms = {el[0]: (el[1], el[2]) for el in valid_segments_ms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (0, 6652313),\n",
       " 2: (0, 6082616),\n",
       " 3: (0, 6130061),\n",
       " 4: (817914, 6053613),\n",
       " 5: (772051, 6430197),\n",
       " 7: (784004, 8033955),\n",
       " 35: (742437, 5647632),\n",
       " 9: (0, 8572196),\n",
       " 10: (0, 5610812),\n",
       " 11: (1286398, 5908218),\n",
       " 12: (807642, 4985841),\n",
       " 13: (1387570, 5626548),\n",
       " 14: (721227, 5936410),\n",
       " 15: (678056, 6131083),\n",
       " 16: (1442156, 2771749),\n",
       " 45: (4288702, 5753146),\n",
       " 17: (755955, 5935093),\n",
       " 18: (768054, 5672919),\n",
       " 19: (690662, 2739184),\n",
       " 20: (751756, 8465912),\n",
       " 21: (0, 9769708),\n",
       " 22: (897660, 5328800),\n",
       " 23: (926244, 8480968),\n",
       " 24: (1161270, 8220734),\n",
       " 25: (1359345, 7725473),\n",
       " 26: (991109, 5510171),\n",
       " 27: (2979805, 5468341),\n",
       " 29: (3157766, 6973012),\n",
       " 30: (3576005, 8426502),\n",
       " 31: (4123547, 5398823),\n",
       " 32: (4216462, 8415540),\n",
       " 33: (4213314, 6638696),\n",
       " 34: (3127441, 6328104)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_segments_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_opts = {\"device\": \"cuda\"}\n",
    "model = separator.from_hparams(source=\"speechbrain/sepformer-whamr-enhancement\", savedir='pretrained_models/sepformer-whamr-enhancement', run_opts=run_opts)\n",
    "\n",
    "audios_path = os.path.join(processed_audio_path, 'normalized')\n",
    "output_path = os.path.join(processed_audio_path, 'denoised')\n",
    "audio_path = os.path.join(processed_audio_path, 'samples/7.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_file(fpath, window_len=60):\n",
    "    batch, fs_file = torchaudio.load(fpath)\n",
    "    pid = int(Path(fpath).stem)\n",
    "    pid_valid_seg = valid_segments_ms[pid]\n",
    "    batch = batch.to(model.device)\n",
    "    fs_model = model.hparams.sample_rate\n",
    "\n",
    "    tf = torchaudio.transforms.Resample(\n",
    "        orig_freq=fs_file, new_freq=fs_model\n",
    "    ).to(model.device)\n",
    "    batch = batch.mean(dim=0, keepdim=True)\n",
    "    batch = tf(batch)\n",
    "\n",
    "    all = []\n",
    "    # cnt = 0\n",
    "    for ini_time in trange(0, batch.shape[1] // fs, window_len):\n",
    "        end_time = ini_time + window_len\n",
    "\n",
    "        if end_time < pid_valid_seg[0]/1000 or ini_time > pid_valid_seg[1]/1000:\n",
    "            # segment is outside of valid seg\n",
    "            all.append(\n",
    "                torch.zeros((1, window_len * fs))\n",
    "            )\n",
    "        else:\n",
    "            ini = ini_time*fs\n",
    "            end = end_time*fs\n",
    "\n",
    "            all.append(\n",
    "                model.separate_batch(batch[:, ini: end]).cpu().squeeze(dim=2)\n",
    "            )\n",
    "        # cnt += 1\n",
    "        # if cnt == 15:\n",
    "        #     break\n",
    "\n",
    "    res = torch.cat(all, axis=1)\n",
    "    res = (\n",
    "        res / res.abs().max(dim=1, keepdim=True)[0]\n",
    "    )\n",
    "    return res.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_and_store(fpath, outpath):\n",
    "    res = denoise_file(fpath).transpose()\n",
    "    wavwrite(outpath, fs, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/data/lared/processed/audio/normalized/16.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8ae96a905846b29ddb2d70aa743fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/data/lared/processed/audio/normalized/45.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411b5c089d4040318cdfbf6d17e80dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fpath in [f for f in Path(audios_path).glob('*.wav') if f.stem in ['16', '45']]:\n",
    "    print(fpath)\n",
    "    out_path = os.path.join(output_path, fpath.name)\n",
    "    denoise_and_store(fpath, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ff63645dd16f55240e07095d3c46f4fac3f89ef16802cfaceca713f6cf38dfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
